### ðŸ” **Exploration**

Trying **new or less-known actions** to discover their rewards.

- Goal: **Learn more** about the environment.
    
- Risk: Might get **lower rewards** short-term.
    
- Benefit: May find **better strategies** long-term.
    


### ðŸ’° **Exploitation**

Choosing the **best-known action** to get the **highest reward** based on what the agent already knows.

- Goal: **Maximize reward now**.
    
- Risk: Might **miss out** on even better actions.
    
- Benefit: Uses **current knowledge** to perform well.
    


### âš–ï¸ **The Dilemma**

- If you **only explore**, you waste time trying bad actions.
    
- If you **only exploit**, you might **never discover better actions**.
    
- A smart agent must **balance both**:
    
    - Learn enough (explore),
        
    - But also use what it has learned (exploit).
        

### ðŸ§  Common Strategy: **Îµ-greedy**

- With probability **Îµ**, explore (random action).
    
- With probability **1 - Îµ**, exploit (best known action).
    

---
### Evolutionary Method
- The least fit members of the population are eliminated. RL agents learn both positive & negative actions but evolutionary

### Immediate RL
- It is a type of reinforcement learning where an agent receives immediate rewards or penalties for its actions at each time step without considering the long-term consequences.
> In Subway Surfer game where a player controls a character to collect coins, the agent receives a positive reward when it runs into an obstacle.