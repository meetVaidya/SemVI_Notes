Recurrent Neural Networks (RNNs) are versatile and can be applied to various sequence processing tasks beyond language modeling, including sequence labeling and sequence classification.

## Sequence Labeling with RNNs

In sequence labeling, the network's task is to assign a label from a small, fixed set of labels to **each element** of an input sequence.

*   **Canonical Examples:**
    *   **Part-of-Speech (POS) Tagging:** Assigning grammatical tags (noun, verb, adjective, etc.) to each word in a sentence.
    *   **Named Entity Recognition (NER):** Identifying and classifying named entities (person, organization, location, etc.) in text.

*   **RNN Approach:**
    *   **Inputs:** Word embeddings for each word in the sequence.
    *   **Outputs:** At each time step, tag probabilities are generated by a softmax layer over the predefined tag set. The tag with the highest probability is chosen as the label for the input word at that time step.
## RNNs for Sequence Classification

In sequence classification, the goal is to classify an **entire sequence** rather than its individual tokens.

*   **Examples:**
    *   Document-level topic classification (e.g., classifying an article as "sports" or "politics").
    *   Spam detection (classifying an email as "spam" or "not spam").
    *   Sentiment analysis (classifying a review as "positive" or "negative").
    *   Deception detection.

*   **RNN Approach:**
    1.  The input text (sequence of words) is passed through the RNN one word at a time.
    2.  At each time step, a new hidden layer $h_t$ is generated.
    3.  The **hidden layer for the final element of the text, $h_n$** (where $n$ is the sequence length), is taken to constitute a compressed representation of the entire sequence.
    4.  This final hidden state $h_n$ serves as the input to a subsequent feedforward network (FFN).
    5.  The FFN then chooses a class, typically via a softmax layer (for multi-class) or a sigmoid layer (for binary class) over the possible classes.

*   **Training for Sequence Classification:**
    *   There are no intermediate outputs or loss terms for the words preceding the last element of the sequence.
    *   The loss function is based entirely on the final text classification task (e.g., comparing the FFN's output with the true label of the sequence).
    *   The error signal from this final classification is backpropagated through the FFN and then through all the weights (W, U, V) of the RNN (Backpropagation Through Time).

## Performing Sentiment Analysis using RNNs (Detailed Steps)

Sentiment analysis involves classifying textual data (e.g., movie reviews, tweets) based on the sentiment expressed (e.g., positive, negative, neutral).

### Step 1: Understanding the Sentiment Analysis Task
*   **Objective:** To classify given text into sentiment categories.
*   **Example Categories:**
    *   Positive sentiment (e.g., "I love this movie")
    *   Negative sentiment (e.g., "I hate this film")
*   **Input:** Text data (reviews, tweets, etc.).
*   **Output:** Sentiment score/class (e.g., binary classification: positive = 1, negative = 0).

### Step 2: Preprocessing the Text Data
Text must be converted into a numerical form before being fed into an RNN.

1.  **Tokenization:**
    *   **Definition:** Splitting a sentence into individual words (tokens).
    *   **Example:**
        *   Input: "This movie is great"
        *   Output tokens: ["this", "movie", "is", "great"]

2.  **Convert Words into Word Embeddings:**
    *   Instead of one-hot encoding, use dense vector representations (word embeddings) like Word2Vec, GloVe, or trainable embeddings.
    *   **Example:**

        | Word  | Word Embedding (Example) |
        |----|----------------------|
        | this  | [0.5, 0.2, 0.1]          |
        | movie | [0.7, 0.3, 0.8]          |
        | is    | [0.2, 0.6, 0.5]          |
        | great | [0.9, 0.4, 0.3]          |
    *   The sentence "This movie is great" becomes a matrix of word embeddings:
        $X = [[0.5, 0.7, 0.2, 0.9], [0.2, 0.3, 0.6, 0.4], [0.1, 0.8, 0.5, 0.3]]^T$ (if embeddings are columns)


### Step 3: Define RNN Model & Forward Pass
The RNN processes the sequence of word embeddings one at a time, maintaining and updating a hidden state $h_t$.

*   **Forward Pass Equation:** $h_t = tanh(W_h * h_{t-1} + W_x * x_t + b_h)$
    *   $x_t$: Word embedding of the current word.
    *   $h_t$: Hidden state at time $t$.
    *   $W_x, W_h, b_h$: Learnable parameters.
*   **Initialization:** $h_0 = [0,0,0]$ (or some other initial vector).

*   **Processing Sequence:**
    *   Time Step 1 (e.g., "This"): $h_1 = tanh(W_h * h_0 + W_x * x_1 + b_h)$
    *   Time Step 2 (e.g., "movie"): $h_2 = tanh(W_h * h_1 + W_x * x_2 + b_h)$
    *   ...
    *   Time Step T (e.g., "great"): $h_T = tanh(W_h * h_{T-1} + W_x * x_T + b_h)$
*   The final hidden state $h_T$ represents the entire sentence's context for sentiment.

### Step 4: Sentiment Prediction
The final hidden state $h_T$ is passed through a fully connected layer and a sigmoid activation function to get the final sentiment score.

*   $y = σ(W_y * h_T + b_y)$
    *   $W_y, b_y$: Learnable parameters of the output layer.
    *   $σ$: Sigmoid function, converts output to a probability between 0 and 1.
*   **Interpretation (for binary classification):**
    *   If $y > 0.5$ → Positive Sentiment
    *   If $y < 0.5$ → Negative Sentiment (or a threshold other than 0.5 can be chosen)

### Step 5: Loss Function & Backpropagation
To train the model, a loss function measures the discrepancy between predicted sentiment and actual sentiment.

*   **Binary Cross-Entropy Loss:**
    $L = -[y_{true} * log(y) + (1 - y_{true}) * log(1 - y)]$
    *   $y_{true}$: Actual sentiment label (1 for positive, 0 for negative).
    *   $y$: Predicted probability.

*   **Backpropagation:** Gradients are computed using Backpropagation Through Time (BPTT), and parameters ($W_x, W_h, b_h, W_y, b_y$) are updated using an optimizer like Gradient Descent.
    $W_{new} = W_{old} - α * (∂L/∂W)$

### Step 6: Model Training
*   Train on a dataset of labeled reviews (e.g., IMDB reviews dataset).
*   Adjust hyperparameters like learning rate, embedding size, and number of RNN units.
*   Use techniques like **dropout** to prevent overfitting.

### Step 7: Model Evaluation
*   Evaluate performance using metrics like accuracy, precision, recall, and F1-score on a test set.
*   Test with new sentences:
    *   "I love this film" → Predicted Sentiment: Positive (1)
    *   "This movie is bad" → Predicted Sentiment: Negative (0)