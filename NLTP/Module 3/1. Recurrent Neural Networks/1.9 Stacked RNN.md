### What Is a Stacked RNN?
- A **stacked RNN** (also called a multi-layer RNN) arranges multiple recurrent layers “on top” of one another.
- At each time step *t*, the input first passes through the bottom RNN layer; its hidden state becomes the input to the next RNN layer, and so on up the stack.

### Architecture
```
Time step t:

Input xₜ ─► [RNN Layer 1] ─► hₜ¹ ─► [RNN Layer 2] ─► hₜ² ─► … ─► [RNN Layer L] ─► hₜᴸ ─► Output or next module
```
- **L** = number of stacked recurrent layers.
- Each layer *ℓ* maintains its own hidden state *hₜ^ℓ*.



### Why Stack RNN Layers?
- **Hierarchical Feature Learning**: Lower layers can capture short-term/low-level patterns; higher layers can learn more abstract, long-range dependencies.
- **Expressive Power**: More layers allow modeling of more complex sequence transformations.



### Trade-Offs
- **Increased Capacity vs. Cost**:
  - More layers ⟶ higher representational power.
  - But each additional layer raises **training time**, **memory usage**, and risk of **overfitting**.
- **Vanishing/Exploding Gradients**:
  - Deeper stacks exacerbate gradient issues; often mitigated with careful initialization, gradient clipping, or using LSTM/GRU cells.
