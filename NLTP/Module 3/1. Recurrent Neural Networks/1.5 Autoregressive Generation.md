### What Is Autoregressive Generation?
- **Definition**: A sequence generation method where each newly generated token is fed back as input for the next time step.
- **Key Characteristic**: At inference time, the model has no access to ground-truth future tokens—predictions are purely conditioned on its own past outputs.
- **Applications**: Text generation, machine translation, poetry/song composition.

---

### Numerical Example: Generating “I love deep learning”

#### Problem Statement
- **Dataset**: Single training sentence:
  > “I love deep learning”
- **Goal**: Train an RNN language model that, at inference, given seed word “I,” will generate the rest of the sentence autoregressively.

#### Step 1: Initialize Model Parameters
- **Hidden state size**: 3
- **Weight matrices** (randomly initialized):
  - Input-to-hidden $W_x$ ($3\times3$)
  - Hidden-to-hidden $W_h$ ($3\times3$)
  - Hidden-to-output $W_y$ ($3\times3$)
- **Bias vectors**: $b_h, b_y$

#### Step 2: Seed the Model
- **Initial hidden state**: $h_0 = [0,0,0]^\top$
- **First input**: Embedding of “I”, e.g. $x_1 = [0.1,\,0.3,\,0.5]^\top$

#### Step 3: Compute First Hidden State
$$
h_1 = \tanh\bigl(W_x\,x_1 \;+\; W_h\,h_0 \;+\; b_h\bigr)
$$
- Since $h_0\!=\!0$, this reduces to $\tanh(W_x x_1 + b_h)$.

#### Step 4: Produce First Output
$$
o_1 = W_y\,h_1 + b_y,\quad
y_1 = \mathrm{SoftMax}(o_1)
$$
- Sample the next word $w_1$ from the distribution $y_1$.

#### Step 5: Continue Autoregressively
1. **Embed** the sampled word $w_1$ to get $x_2$.
2. **Compute**
   $$
     h_2 = \tanh(W_x\,x_2 + W_h\,h_1 + b_h),\quad
     y_2 = \mathrm{SoftMax}(W_y\,h_2 + b_y)
   $$
3. **Sample** $w_2$ from $y_2$.
4. **Repeat** until an end-of-sentence token `</s>` or length limit is reached.

---

### Why Autoregressive?
- **Contextual Coherence**: Each choice conditions on all previously generated tokens, promoting consistency.
- **Flexibility**: Can generate variable-length outputs, stopping dynamically when an end token is produced.