### What Is Sequence Labelling?
- **Definition**: Assign a label from a fixed tag set to each element of an input sequence.
- **Common Tasks**:
  - **Part-of-Speech (POS) Tagging**
  - **Named Entity Recognition (NER)**

### RNN-Based Sequence Labelling Architecture
1. **Input Layer**
   - Each token $w_t$ is converted to an embedding $x_t$.
2. **Recurrent Layer**
   $$
     h_t = \tanh\bigl(U\,h_{t-1} \;+\; W\,x_t \;+\; b_h\bigr)
   $$
3. **Output Layer (per time step)**
   $$
     o_t = V\,h_t + b_o,\quad
     y_t = \mathrm{SoftMax}(o_t)
   $$
   - $y_t$ is a probability distribution over the tag set.
4. **Loss Function**
   - Sum of cross-entropy losses across all time steps:
     $$
       L = -\sum_{t=1}^{T} \log P_{\theta}(t_t \mid w_{1:t})
     $$
   where $t_t$ is the true tag at position $t$.

### Part-of-Speech Tagging Example
| Token       | Embedding $x_t$ | Hidden State $h_t$ | Tag Distribution $y_t$ | Predicted Tag |
|-------------|-------------------|----------------------|---------------------------|---------------|
| “The”       | $\mathbf{x}_1$  | $\mathbf{h}_1$     | [DET:0.90, …]             | DET           |
| “cat”       | $\mathbf{x}_2$  | $\mathbf{h}_2$     | [NOUN:0.85, …]            | NOUN          |
| “sat”       | $\mathbf{x}_3$  | $\mathbf{h}_3$     | [VERB:0.88, …]            | VERB          |

### Key Points
- **Per-step Prediction**: Unlike many-to-one RNNs, sequence labelling produces an output at **every** time step.
- **Backpropagation Through Time**: Gradients flow through all time steps, updating $U, W, V$ based on every tag decision.
- **Advantages**:
  - Captures context both from earlier tokens (via $h_{t-1}$).
  - Learns task-specific embedding and tag predictors jointly.
- **Considerations**:
  - Long sequences may exacerbate vanishing/exploding gradients—often mitigated with LSTM/GRU cells.