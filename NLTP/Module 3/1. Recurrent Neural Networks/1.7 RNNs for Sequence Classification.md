### Task Definition
- **Many-to-one** architecture: Input is a sequence of tokens $\{w_1, w_2, \dots, w_T\}$; output is a single class label (e.g., topic, spam/ham, sentiment).

### Model Architecture
1. **Embedding & Recurrence**
   - Each token $w_t$ â†’ embedding $x_t$.
   - Hidden state update for $t=1\ldots T$:
     $$
       h_t = \tanh\bigl(U\,h_{t-1}+W\,x_t + b_h\bigr)
     $$
2. **Sequence Representation**
   - Use **final hidden state** $h_T$ as a summary of the entire sequence.
3. **Classification Layer**
   $$
     o = V\,h_T + b_o,\quad
     y = \mathrm{SoftMax}(o)
   $$
   - $y$ is a distribution over the set of possible classes.
4. **Loss Function**
   - Cross-entropy between $y$ and the true label $c$:
     $$
       L = -\log P_\theta(c \mid w_{1:T})
     $$

### Training Dynamics
- **Backpropagation Through Time (BPTT)** propagates the classification error from the output back through all time steps, updating $U,W,V$.
- Only the **final** output incurs a loss; no intermediate losses at $t < T$.

### Common Applications
- **Document-level Topic Classification**
- **Spam Detection**
- **Customer-service Routing**
- **Deception Detection**
- **Sentiment Classification**