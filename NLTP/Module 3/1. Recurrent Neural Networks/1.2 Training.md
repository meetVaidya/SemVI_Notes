### Objective

- Like other neural networks, RNNs are trained to minimize a **loss function** over a dataset.
- But due to their sequential nature, training involves **handling dependencies across time steps**.


For each time step, the RNN computes the hidden state and output using three core sets of weights:

- **W**: input-to-hidden weights
- **U**: hidden-to-hidden (recurrent) weights
- **V**: hidden-to-output weights

These are **shared** across time steps.

### Dependencies

- The hidden state at time *t*, denoted as *hₜ*, depends on $h_{t-1}$, which itself depends on $h_{t-2}$, and so on…
- Therefore, **the loss at a single time step depends on multiple previous time steps.**
- Consequently, we must consider **all time steps** to compute gradients properly.

---

### Backpropagation Through Time (BPTT)

^fd7fd2

A special form of backpropagation designed for sequences.

#### Step 1: Forward Pass

- Input sequence processed step by step:
  - Compute $h_t$, $y_t$
  - Save intermediate values (e.g., $h_t$) for backprop
- Accumulate loss over all time steps.

#### Step 2: Backward Pass

- Go **back in time**, computing gradients at each step:
  - Gradients flow from later time steps (*t+1*) to earlier (*t-1*)
  - Error is backpropagated through both time and the hidden layers
- Gradients for W, U, V are calculated using the **chain rule**, considering their impact across all time steps.

### Example

If your sequence has T time steps:

$$
L = \sum_{t=1}^{T} L_t
$$

The gradient of loss with respect to U, for example, is:

$$
\frac{\partial L}{\partial U} = \sum_{t=1}^{T} \frac{\partial L}{\partial h_t} \cdot \frac{\partial h_t}{\partial U}
$$

### Why Is BPTT Challenging?

- **Exploding gradients**: gradients become too large → unstable training
- **Vanishing gradients**: gradients shrink to near zero → learning stalls
  - Especially problematic for **long sequences**
  - Solution: Use **LSTM** or **GRU** architectures (covered later)
