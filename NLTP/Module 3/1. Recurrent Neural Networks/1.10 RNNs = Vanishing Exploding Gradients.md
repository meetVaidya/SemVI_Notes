### The Problem
- During BPTT, gradients are propagated multiplicatively through each time step.
- If the derivatives of the activation (e.g., tanh or sigmoid) are **< 1**, repeated multiplication causes gradients to **shrink exponentially** → **vanishing gradients**.
- If the derivatives are **> 1**, gradients can **grow exponentially** → **exploding gradients**.

---

### Consequences
- **Vanishing Gradients**:
  - Early time-step dependencies cannot be learned.
  - Long-range information is effectively “forgotten.”
- **Exploding Gradients**:
  - Training becomes unstable—loss can diverge.
  - Parameter updates may become excessively large.

---

### Solutions & Mitigations
1. **Gradient Clipping**
   - Scale down gradients when their norm exceeds a threshold.
   - Controls exploding gradients without altering small ones.
2. **Careful Initialization**
   - Use orthonormal or scaled initial weight matrices to keep eigenvalues near 1.
3. **Alternative Activations**
   - Replace tanh/sigmoid with **ReLU** (derivative 0 or 1) to reduce vanishing.
4. **Gated Architectures**
   - **LSTM** and **GRU** cells introduce gates and a memory cell that preserve error flow, effectively combating both issues (covered next).