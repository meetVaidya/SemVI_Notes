### Why Word Embeddings?
- **One-hot vectors** are high-dimensional (size = vocabulary |V|) and sparse.
- They treat each word as equally distant—no notion of similarity.
- **Embeddings** map words to dense, low-dimensional vectors that capture semantic relationships.

---

### Example Setup
- Vocabulary $V = \{\mathrm{The},\,\mathrm{cat},\,\mathrm{sat},\,\mathrm{on},\,\mathrm{mat}\}$
- Embedding dimension $d = 3$

| Word | Embedding Vector $[e_1,e_2,e_3]$ |
|------|--------------------------------------|
| The  | [0.4, 0.1, 0.7]                      |
| cat  | [0.9, 0.3, 0.8]                      |
| sat  | [0.5, 0.2, 0.6]                      |
| on   | [0.2, 0.7, 0.3]                      |
| mat  | [0.8, 0.5, 0.1]                      |

---

### How It Works
1. **Embedding Lookup**:
   - Input one-hot $x_t$ selects a row from the embedding matrix $E$.
   - $e_t = E\,x_t$ yields the corresponding dense vector.
2. **Reduced Dimensionality**:
   - Instead of feeding a $|V|$-dimensional one-hot into the RNN, you feed the $d$-dimensional $e_t$.
3. **Semantic Encoding**:
   - Similar words (e.g., “cat” and “mat” in language of this toy example) can have nearby vectors in $\mathbb{R}^3$.
4. **Learned Representations**:
   - $E$ is a parameter of the model. During training, embeddings adjust to capture task-relevant relationships.