Standard RNNs struggle to retain information over long sequences due to vanishing/exploding gradients. **LSTM** and **GRU** architectures introduce gating mechanisms that regulate information flow, enabling the network to learn long-range dependencies more effectively.
### Long Short-Term Memory (LSTM)

**Core Components**:
- **Cell state** $c_t$: A “conveyor belt” carrying information forward largely unchanged.
- **Gates** (sigmoid activations) that control what to **forget**, what new information to **add**, and what to **output**:

| Gate                        | Equation                                                      | Function                                                      |
| --------------------------- | ------------------------------------------------------------- | ------------------------------------------------------------- |
| **Forget** $f_t$            | $f_t = \sigma(W_f\,x_t + U_f\,h_{t-1} + b_f)$                 | Decides which parts of $c_{t-1}$ to erase (0=forget, 1=keep). |
| **Input** $i_t$             | $i_t = \sigma(W_i\,x_t + U_i\,h_{t-1} + b_i)$                 | Controls which new candidate values to write.                 |
| **Candidate** $\tilde{c}_t$ | $\tilde{c}_t = \tanh(W_c\,x_t + U_c\,h_{t-1} + b_c)$          | Potential new content to add to cell state.                   |
| **Cell update**             | $c_t = f_t \,\odot\, c_{t-1} \;+\; i_t \,\odot\, \tilde{c}_t$ | Combines old state and new candidate.                         |
| **Output** $o_t$            | $o_t = \sigma(W_o\,x_t + U_o\,h_{t-1} + b_o)$                 | Decides which parts of $c_t$ to expose.                       |
| **Hidden state** $h_t$      | $h_t = o_t \,\odot\, \tanh(c_t)$                              | Final output for this time step.                              |

> The forget gate $f_t$ allows the model to drop irrelevant information; the input and candidate computations add new, salient information. The cell state $c_t$ thus forms a highway for gradient flow, alleviating vanishing gradients.
### Gated Recurrent Unit (GRU)

**GRU** streamlines LSTM by combining the forget & input gates into a single **update gate**, and merging cell & hidden states:

| Gate                        | Equation                                                            | Function                                                                            |
| --------------------------- | ------------------------------------------------------------------- | ----------------------------------------------------------------------------------- |
| **Update** $z_t$            | $z_t = \sigma(W_z\,x_t + U_z\,h_{t-1} + b_z)$                       | Controls interpolation between old state $h_{t-1}$ and new candidate $\tilde{h}_t$. |
| **Reset** $r_t$             | $r_t = \sigma(W_r\,x_t + U_r\,h_{t-1} + b_r)$                       | Determines how much past state to forget when computing $\tilde{h}_t$.              |
| **Candidate** $\tilde{h}_t$ | $\tilde{h}_t = \tanh(W\,x_t + U\,[r_t \,\odot\, h_{t-1}] + b)$      | New candidate hidden state based on reset-modified past.                            |
| **Hidden state** $h_t$      | $h_t = z_t \,\odot\, h_{t-1} \;+\; (1 - z_t) \,\odot\, \tilde{h}_t$ | Weighted blend of old state and candidate.                                          |

> GRUs are often faster to train (fewer parameters) while retaining much of the LSTM’s ability to capture long-term dependencies.
