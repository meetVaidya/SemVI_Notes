Recurrent Neural Networks (RNNs) are specialized deep learning models designed to process sequential data while maintaining temporal context through recurrent connections. Here's a structured explanation of their key components:

## Motivation  
- **Sequential data ubiquity**: RNNs excel with inherently ordered data like text, speech, and time-series where element sequence carries crucial meaning. 
- **Limitations of traditional methods**: Bag-of-words and fixed-window approaches discard positional information, reducing performance on context-dependent tasks like language translation.

## Core Idea  
- **Recurrent processing**: At each timestep $t$, the network:  
  - Receives current input $x_t$  
  - Integrates previous hidden state $h_{t-1}$  
  - Produces updated hidden state *hₜ* containing historical context.  
- **Weight sharing**: Same parameters (U, W) are reused across all timesteps, enabling pattern recognition in variable-length sequences.  

## Hidden (Memory) State  
- **Context accumulator**: The hidden state $h_t$ acts as compressed memory of all previous inputs through recursive updates.  
- **Mathematical representation**:  
  $$
    hₜ = \tanh\bigl(U\,hₜ₋₁ \;+\; W\,xₜ \;+\; b\bigr)
  $$  
  Where *U* (hidden-to-hidden), *W* (input-to-hidden), and *b* (bias) are learnable parameters.  

## Architecture Components  
```
    xₜ  ──►[ W ]──► hₜ ──►[ V ]──► yₜ
             ▲
             │
           [ U ]
             │
           hₜ₋₁
```
- **Recurrent cell**: Processes one sequence element per timestep through three core components.
- **Parameter sharing**: Matrix *U* enables temporal information flow between hidden states.  

## Temporal Unrolling  
```
x₁ → h₁ → y₁
      │
x₂ → h₂ → y₂
      │
...  
      │
xₙ → hₙ → yₙ
```
- **Training mechanism**: The network is conceptually unrolled through time for [[1.2 Training#^fd7fd2|backpropagation (BPTT)]], allowing gradient flow across sequence steps.
- **Memory efficiency**: Actual implementation uses shared parameters rather than physical unrolling 

This architecture enables RNNs to model temporal dynamics while maintaining fixed parameter counts, making them suitable for variable-length sequential data processing.