Recurrent Neural Networks (RNNs) are specialized deep learning models designed to process sequential data while maintaining temporal context through recurrent connections[1][3][4]. Here's a structured explanation of their key components:

## Motivation  
- **Sequential data ubiquity**: RNNs excel with inherently ordered data like text, speech, and time-series where element sequence carries crucial meaning[1][4][5]  
- **Limitations of traditional methods**: Bag-of-words and fixed-window approaches discard positional information, reducing performance on context-dependent tasks like language translation[1][3][4]  

## Core Idea  
- **Recurrent processing**: At each timestep *t*, the network:  
  - Receives current input *xₜ*  
  - Integrates previous hidden state *hₜ₋₁*  
  - Produces updated hidden state *hₜ* containing historical context[4][6][13]  
- **Weight sharing**: Same parameters (U, W) are reused across all timesteps, enabling pattern recognition in variable-length sequences[4][5][7]  

## Hidden (Memory) State  
- **Context accumulator**: The hidden state *hₜ* acts as compressed memory of all previous inputs through recursive updates[6][7][13]  
- **Mathematical representation**:  
  $$
    hₜ = \tanh\bigl(U\,hₜ₋₁ \;+\; W\,xₜ \;+\; b\bigr)
  $$  
  Where *U* (hidden-to-hidden), *W* (input-to-hidden), and *b* (bias) are learnable parameters[4][5][11]  

## Architecture Components  
```
    xₜ  ──►[ W ]──► hₜ ──►[ V ]──► yₜ
             ▲
             │
           [ U ]
             │
           hₜ₋₁
```
- **Recurrent cell**: Processes one sequence element per timestep through three core components[1][4]  
- **Parameter sharing**: Matrix *U* enables temporal information flow between hidden states[5][7][11]  

## Temporal Unrolling  
```
x₁ → h₁ → y₁
      │
x₂ → h₂ → y₂
      │
...  
      │
xₙ → hₙ → yₙ
```
- **Training mechanism**: The network is conceptually unrolled through time for backpropagation (BPTT), allowing gradient flow across sequence steps[1][5][7]  
- **Memory efficiency**: Actual implementation uses shared parameters rather than physical unrolling 

This architecture enables RNNs to model temporal dynamics while maintaining fixed parameter counts, making them suitable for variable-length sequential data processing.