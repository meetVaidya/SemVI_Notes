### What Is Language Modeling?
- **Definition**: Assigns a probability to a sequence of words or predicts the next word given a context.
- **Goal**: Learn $P(w_t \mid w_1, w_2, \dots, w_{t-1})$.

---

### Traditional N-gram Models
- Use fixed-size context (e.g. 2- or 3-word windows).
- **Limitations**:
  - Poor at capturing long-range dependencies.
  - High model complexity for large $n$.

---

### Neural Language Models & RNNs
- Neural networks learn dense representations and can generalize better.
- **RNN Advantage**: Hidden state carries information from *all* prior words, avoiding fixed-window limits .

---

### RNNs as Language Models
1. **Input**: Sequence $\{x_1, x_2, \dots, x_T\}$, where each $x_t$ is a one-hot vector of size $|V|$.
2. **Embedding Lookup**: $e_t = E\,x_t$, where $E$ is the embedding matrix.
3. **Hidden-state update**:
   $$
     h_t = \tanh\bigl(U\,h_{t-1} + W\,e_t + b_h\bigr)
   $$
4. **Output logits**:
   $$
     o_t = V\,h_t + b_o
   $$
5. **SoftMax**:
   $$
     y_t = \mathrm{SoftMax}(o_t)\quad\text{→ probability over \(|V|\) words.}
   $$

---

### Training RNN Language Models
- **Loss**: Sum of cross-entropy losses over all time steps:
  $$
    L = -\sum_{t=1}^T \log P_{\theta}(w_t \mid w_{<t})
  $$
- **Optimization**: Backpropagation Through Time (BPTT) updates $U, W, V$ by accumulating gradients from each step .
- **Challenges**: Vanishing/exploding gradients—mitigated later by LSTM/GRU architectures.

---

### Text Generation with RNN Models
1. **Initialization**: Start with a special token `<s>`.
2. **Sampling Loop**:
   - Compute $y_1=\mathrm{SoftMax}(V\,h_1+b_o)$.
   - Sample a word $w_1$ from $y_1$.
   - Embed $w_1$ and feed into next step.
1. **Termination**: Stop when `</s>` is generated or max length reached.