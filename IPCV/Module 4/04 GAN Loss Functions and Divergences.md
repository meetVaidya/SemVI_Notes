## 1. GAN Loss: General Concept
    - **Definition**: In GANs, "loss" refers to the **objective functions** used to train the Generator (G) and the Discriminator (D).
    - **Two Networks, Two Losses**: Since GANs involve two networks competing, they have distinct (though related) loss functions.
    - **Common Basis (Original GAN)**: Often based on **Binary Cross-Entropy (BCE) Loss**, but adapted for the adversarial roles.

## 2. Original GAN Loss (Minimax Loss - Goodfellow et al., 2014)
    - **The Game**: A two-player minimax game where:
        - **Discriminator (D)** tries to **maximize** its ability to correctly distinguish real data (`x ~ p_data(x)`) from fake data (`G(z)`, where `z ~ p_z(z)`).
        - **Generator (G)** tries to **minimize** the Discriminator's ability to detect its fakes, effectively trying to make `D(G(z))` close to 1 (look real).
    - **Value Function `V(D, G)`**:
      `min_G max_D V(D, G) = E_{x~p_data(x)}[log D(x)] + E_{z~p_z(z)}[log(1 - D(G(z)))]`
    - **Breakdown**:
        - `min_G`: Generator minimizes the value function.
        - `max_D`: Discriminator maximizes the value function.
        - `E_{x~p_data(x)}[log D(x)]`: Expected log-likelihood that real data `x` is classified as real by D. D wants `D(x)` to be close to 1 (so `log D(x)` is close to 0, maximizing this term).
        - `E_{z~p_z(z)}[log(1 - D(G(z)))]`: Expected log-likelihood that fake data `G(z)` is classified as fake by D. D wants `D(G(z))` to be close to 0 (so `1 - D(G(z))` is close to 1, and `log(1 - D(G(z)))` is close to 0, maximizing this term).
    - **Discriminator's Training**:
        - Goal: Maximize `V(D,G)`.
        - Loss `L_D = - (log D(x) + log(1 - D(G(z))))`. (Minimize negative of terms it wants to maximize).
        - Wants `D(x) -> 1` (real is real) and `D(G(z)) -> 0` (fake is fake).
    - **Generator's Training**:
        - Goal: Minimize `V(D,G)`. In practice, minimizing `log(1 - D(G(z)))` can lead to vanishing gradients early in training if D is strong.
        - Alternative G Loss (Non-Saturating): Maximize `log D(G(z))`, which is equivalent to minimizing `L_G = -log D(G(z))`.
        - Wants `D(G(z)) -> 1` (fool D into thinking fake is real).
    - **Numeric Example (Conceptual)**:
        - If D predicts `D(real_image) = 0.9` and `D(fake_image) = 0.3`:
            - D is doing reasonably well (high score for real, low for fake).
            - D's loss for real: `-log(0.9) ≈ 0.105`. D's loss for fake: `-log(1-0.3) = -log(0.7) ≈ 0.357`. Total D loss (sum) is low.
            - G's loss (non-saturating): `-log(0.3) ≈ 1.203`. G's loss is high because D easily spotted the fake.
        - As G improves, `D(fake_image)` should increase, reducing G's loss.
        - As D improves, its ability to separate real/fake increases, initially increasing G's loss.

## 3. Jensen-Shannon Divergence (JSD)
    - **Definition**: A method to measure the **similarity (or difference) between two probability distributions**. It's a symmetrized and smoothed version of Kullback-Leibler (KL) Divergence.
    - **In GAN Context**:
        - Used to quantify the difference between the true data distribution (`P_data`) and the distribution of data generated by the Generator (`P_G`).
        - The **goal of GAN training** (from a theoretical perspective) is to make `P_G` as close as possible to `P_data`, which means **minimizing the JSD** between them.
        - It can be shown that the original GAN minimax objective function, when the discriminator is optimal, minimizes `2 * JSD(P_data || P_G) - 2 * log(2)`.
    - **Formula**:
      `JSD(P || Q) = 1/2 * KL(P || M) + 1/2 * KL(Q || M)`
      Where `M = 1/2 * (P + Q)` is the mixture distribution.
    - **Kullback-Leibler (KL) Divergence**:
        - `KL(P || Q) = Σ P(x) * log(P(x) / Q(x))`
        - Measures how much "extra information" (in bits or nats) is needed to represent samples from P using a code optimized for Q.
        - Asymmetric: `KL(P || Q) != KL(Q || P)`.
        - `KL(P || Q) >= 0`, and `KL(P || Q) = 0` if and only if `P = Q`.
    - **JSD Properties**:
        - Symmetric: `JSD(P || Q) = JSD(Q || P)`.
        - Always non-negative. `JSD = 0` if P=Q.
        - Bounded (e.g., by `log(2)` for binary outcomes).
    - **Example Calculation (Discrete Distributions)**:
        - `P = [0.5, 0.3, 0.2]` (Real data distribution)
        - `Q = [0.4, 0.4, 0.2]` (Generated data distribution)
        1.  `M = 0.5 * (P + Q) = [0.45, 0.35, 0.2]`
        2.  `KL(P || M) = 0.5*log(0.5/0.45) + 0.3*log(0.3/0.35) + 0.2*log(0.2/0.2) ≈ 0.002`
        3.  `KL(Q || M) = 0.4*log(0.4/0.45) + 0.4*log(0.4/0.35) + 0.2*log(0.2/0.2) ≈ 0.004` (Note: slide has error, should be 0.0046 or similar)
        4.  `JSD(P || Q) = 0.5 * (0.002 + 0.004) = 0.003` (Using slide's intermediate values)
        - A small JSD value indicates the distributions are similar.

## 4. Other GAN Loss Functions (Brief Mention)
    - *The original GAN loss (BCE/minimax) can be unstable. Many alternatives have been proposed to improve stability and sample quality.*
    - **Least Squares GAN (LSGAN)**: Uses L2 (mean squared error) loss instead of BCE. Can provide more stable gradients.
    - **Wasserstein GAN (WGAN)**:
        - Uses **Wasserstein distance (Earth Mover's Distance - EM Distance)** as the loss.
        - Wasserstein distance measures the "cost" of transforming one distribution into another.
        - Provides smoother gradients, helps with vanishing gradient problem and mode collapse.
        - Discriminator (called "critic" in WGAN) outputs a score rather than a probability.
        - Requires the critic to be **Lipschitz continuous** (often enforced by weight clipping or gradient penalty).
    - **WGAN with Gradient Penalty (WGAN-GP)**: Improves upon WGAN by using a gradient penalty to enforce the Lipschitz constraint instead of weight clipping, leading to more stable training.
    - **Hinge Loss GAN**: Inspired by SVMs, uses hinge loss for the discriminator. Can produce sharper images.
    - **Relativistic GAN (R-GAN)**: Discriminator estimates the probability that real data is more realistic than fake data, rather than classifying them independently.
    - **Logits-based Loss (StyleGAN)**: Uses logistic loss on logits for more stable training.
    - **Contrastive Learning Loss (StyleGAN2-ADA)**: Uses adaptive discriminator augmentation (ADA) and contrastive losses to prevent overfitting, especially with limited data.